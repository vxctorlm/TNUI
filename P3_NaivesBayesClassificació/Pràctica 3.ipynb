{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes i Classificació\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest tercer lliurament es programarà un classificador, que donada una *piulada*, la categoritzarà en una de les possibles classes. En aquesta ocasió, implementareu un classificador amb *piulades* relacionades amb el *cyber bullying*.\n",
    "\n",
    "Recordeu que l'objectiu principal d'aquesta pràctica és desenvolupar un classificador amb una *accuracy* alta. \n",
    "\n",
    "\n",
    "**Què s’ha de fer?**\n",
    "\n",
    "Volem classificar *piulades* segons a quin tipus de *cyber bullying* pertanyen. Així doncs, a partir de tots les *piulades* que tenim, crearem un vector de característiques que ens descrigui cadascuna. A continuació desenvoluparem un classificador probabilístic del tipus **Naive Bayes** que ens permeti identificar a quina classe de *cyber bullying* pertany una *piulada* donada, segons les característiques disenyades.\n",
    "\n",
    "\n",
    "**Quina és la idea del sistema de classificació que s’ha de desenvolupar?**\n",
    "\n",
    "El classificador és un concepte de l'aprenentatge automàtic supervisat. L'objectiu del classificador és donat un vector de característiques que descriuen els objectes que es volen classificar indicar a quina categoria o classe pertanyen d'entre un conjunt predeterminat. \n",
    "\n",
    "El procés de classificació consta de dues parts: \n",
    "\n",
    "+ el procés d'aprenentatge i \n",
    "+ el procés d'explotació o testeig. \n",
    "\n",
    "El procés d'aprenentatge rep exemples de parelles $(x,y)$ on $x$ són les característiques, usualment representades per nombres reals, i $y$ és la categoria a la que pertanyen. \n",
    "Aquest conjunt se'l coneix com a conjunt d'entrenament i ens servirà per trobar una funció $\\hat{y}=f(x)$ que donada una $x$ aconsegueixi que $\\hat{y}$ sigui semblant $y$. \n",
    "\n",
    "Per altra banda el procés de testeig aplica la funció $f(x)$ apresa a l'entrenament a a dades no presents en el conjunt d'aprenentatge per avaluar el classificador.\n",
    "\n",
    "**Classificació i llenguatge natural**\n",
    "\n",
    "La descripció dels exemples en característiques és el punt més crític de tot sistema d'aprenentatge automàtic. \n",
    "Una de les representacions més simples per tal de descriure un text és la representació [\"bag-of-words\"](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "Aquesta representació converteix un text en un vector de $N$ paraules. \n",
    "Primer haurem de seleccionar el conjunt d'$N$ paraules que volem fer servir. Després, per cada paraula comptar quants cops apareix en el text. \n",
    "\n",
    "Una versió alternativa i més simple d'aquest procés pot ser simplement indicar al vector si una determinada paraula apareix o no en el text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abans de començar\n",
    "\n",
    "\n",
    "**\\+ Durant la pràctica, solament es podran fer servir les següents llibreries**:\n",
    "\n",
    "`Pandas, Numpy, Re` i `NLTK`\n",
    "\n",
    "Python té un paquet integrat anomenat [\"re\"](https://www.w3schools.com/python/python_regex.asp), que es pot utilitzar per treballar amb expressions regulars. És molt aconsellable fer servir aquest paquet per processar les paraules.\n",
    "\n",
    "**\\+ No es poden modificar les definicions de les funcions donades, ni canviar els noms de les variables i paràmetres ja donats**\n",
    "\n",
    "Això no implica però que els hàgiu de fer servir. És a dir, que la funció tingui un paràmetre anomenat `df` no implica que l'hàgiu de fer servir, si no ho trobeu convenient.\n",
    "\n",
    "**\\+ En les funcions, s'especifica que serà i de quin tipus cada un dels paràmetres, cal respectar-ho**\n",
    "\n",
    "Per exemple (ho posarà en el pydoc de la funció), `df` sempre serà indicatiu del `Pandas.DataFrame` de les dades. Durant els testos, els paràmetres (i específicament `df`) no contindran les mateixes dades que en aquest notebook, si bé si seran del mateix tipus! Per tant, no us refieu de què tinguin, per exemple, el mateix nombre de files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les dades\n",
    "\n",
    "El 15 d'Abril de 2020, UNICEF va llançar una alarma com a resposta de l'augment de risc de *cyber bullying* durant la pandèmia COVID-19. \n",
    "\n",
    "Les estadístiques són prou alarmants: un 36.5% dels estudiants de l'escola fins a l'institut s'han sentit víctimes del *cyber bullying* i un 87% n'han estat testimonis, amb efectes que van des d'una disminució de resultats acadèmics fins a pensaments suïcides.\n",
    "\n",
    "Amb l'objectiu d'ajudar a l'analisis de la situació, s'ha construit un dataset que conté més de 47000 *piulades* etiquetades d'acord amb la classe de *cyber bullying* que s'està donant:\n",
    "\n",
    "1. `age`;\n",
    "2. `ethnicity`;\n",
    "3. `gender`;\n",
    "4. `religion`;\n",
    "5. `other type of cyberbullying`;\n",
    "6. `not cyberbullying`\n",
    "\n",
    "Les dades han estat balancejades per tal de contenir aproximadament 8000 mostres de cada classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar les dades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de les dades"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-14T22:28:38.435414Z",
     "start_time": "2024-12-14T22:28:37.381526Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-14T22:28:39.362941Z",
     "start_time": "2024-12-14T22:28:39.054466Z"
    }
   },
   "source": [
    "df = pd.read_csv('./cyberbullying_tweets.csv')\n",
    "df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              tweet_text cyberbullying_type\n",
       "0      In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1      Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2      @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3      @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4      @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying\n",
       "...                                                  ...                ...\n",
       "47687  Black ppl aren't expected to do anything, depe...          ethnicity\n",
       "47688  Turner did not withhold his disappointment. Tu...          ethnicity\n",
       "47689  I swear to God. This dumb nigger bitch. I have...          ethnicity\n",
       "47690  Yea fuck you RT @therealexel: IF YOURE A NIGGE...          ethnicity\n",
       "47691  Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...          ethnicity\n",
       "\n",
       "[47692 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47687</th>\n",
       "      <td>Black ppl aren't expected to do anything, depe...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47688</th>\n",
       "      <td>Turner did not withhold his disappointment. Tu...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47689</th>\n",
       "      <td>I swear to God. This dumb nigger bitch. I have...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47690</th>\n",
       "      <td>Yea fuck you RT @therealexel: IF YOURE A NIGGE...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47691</th>\n",
       "      <td>Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47692 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T22:28:11.200845800Z",
     "start_time": "2024-12-14T09:37:19.738116Z"
    }
   },
   "source": [
    "df['cyberbullying_type'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cyberbullying_type\n",
       "religion               7998\n",
       "age                    7992\n",
       "gender                 7973\n",
       "ethnicity              7961\n",
       "not_cyberbullying      7945\n",
       "other_cyberbullying    7823\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparació del dataset\n",
    "\n",
    "Dividim les *piulades* en un conjunt d'entrenament, *train*, i en un conjunt de validació, *test*, per tal de poder entrenar i validar el nostre model de classificació."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T22:28:48.078271Z",
     "start_time": "2024-12-14T22:28:42.433827Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_tweets_train, df_tweets_test = train_test_split(df, test_size=0.2)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com les dades estaven balancejades originalment, podem observar que la distribució de cadascuna de les classes es manté:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T22:28:11.221051900Z",
     "start_time": "2024-12-14T09:37:20.039634Z"
    }
   },
   "source": [
    "df_tweets_train['cyberbullying_type'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cyberbullying_type\n",
       "gender                 6451\n",
       "not_cyberbullying      6377\n",
       "religion               6370\n",
       "ethnicity              6369\n",
       "age                    6361\n",
       "other_cyberbullying    6225\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T22:28:11.229055600Z",
     "start_time": "2024-12-14T09:37:20.220354Z"
    }
   },
   "source": [
    "df_tweets_test['cyberbullying_type'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cyberbullying_type\n",
       "age                    1631\n",
       "religion               1628\n",
       "other_cyberbullying    1598\n",
       "ethnicity              1592\n",
       "not_cyberbullying      1568\n",
       "gender                 1522\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementació\n",
    "\n",
    "Dividirem el notebook en 3 seccions que es complementen una a l'altra:\n",
    "\n",
    "1. Anàlisi exploratòria de les dades: Informació bàsica sobre les *piulades*.\n",
    "2. Processament de les dades: Creació d'un vector de característiques a partir de les *piulades*.\n",
    "3. Desenvolupament d'un classificador *Naive Bayes*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Anàlisis de dades\n",
    "\n",
    "El primer que haurem de fer és analitzar les dades per veure una mica com són. El que us proposem és fer una sèrie de visualitzacions per observar les dades, com ara:\n",
    "\n",
    "* el nombre de *piulades* que s'estan dirigint a una persona en concret\n",
    "* el nombre de  *hashtags* que hi ha a cada categoria de *piulades*\n",
    "* el nombre de  *piulades* que hi ha de cada categoria de *piulades*\n",
    "* el nombre de  *piulades* de la categoria `not_cyberbullying` que es dirigeixen a un usuari vs totes les altres categories\n",
    "* altres coses que penseu que poden ser rellevants."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T22:28:11.239336300Z",
     "start_time": "2024-12-14T09:37:20.369391Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-14T22:28:11.242732Z",
     "start_time": "2024-12-14T09:37:20.569145Z"
    }
   },
   "source": [
    "df_tweets_train.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              tweet_text   cyberbullying_type\n",
       "40252  @stephenasmith fuck you nigga. Or nigger. What...            ethnicity\n",
       "29381  @ibogost SAVE SOME RIBBONS FOR ME. &lt;3 &lt;3...  other_cyberbullying\n",
       "22973  LGBTQ rights should have nothing to do with po...             religion\n",
       "26401  @taitaisanchez i hate cardio. lifting is a lot...  other_cyberbullying\n",
       "25232  Even if they asked for permission - and who kn...  other_cyberbullying"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40252</th>\n",
       "      <td>@stephenasmith fuck you nigga. Or nigger. What...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29381</th>\n",
       "      <td>@ibogost SAVE SOME RIBBONS FOR ME. &amp;lt;3 &amp;lt;3...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22973</th>\n",
       "      <td>LGBTQ rights should have nothing to do with po...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26401</th>\n",
       "      <td>@taitaisanchez i hate cardio. lifting is a lot...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25232</th>\n",
       "      <td>Even if they asked for permission - and who kn...</td>\n",
       "      <td>other_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 1: \n",
    "\n",
    "* Quantes *piulades* estan dirigides a un usuari concret? Diem que una piulada va dirigida a una persona en concret si contenen el patró ``@usuari``. \n",
    "\n",
    "[Restriccions](https://help.twitter.com/en/managing-your-account/change-twitter-handle) que imposa X sobre el patró:\n",
    "+ Nomes són valids aquells patrons amb caràcters alfanumèrics ``[A-z 0-9]`` i el caràcter ``_``\n",
    "+ Els patrons han de començar per ``@`` i han de tenir més de 4 i menys de 16 caràcters.\n",
    "\n",
    "En aquest exercici és extremadament útil l'ús del mòdul ``re``."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T22:28:11.242732Z",
     "start_time": "2024-12-14T09:37:20.742884Z"
    }
   },
   "source": [
    "# # El vostre codi aquí\n",
    "# # Patró per detectar usuaris\n",
    "# # patro_usuari = r'@[A-Za-z0-9_]{4,15}'\n",
    "# # df['dirigida_a_usuari'] = df['tweet_text'].apply(lambda text: bool(re.search(patro_usuari, str(text))))\n",
    "\n",
    "# # nombre_piulades_dirigides = df['dirigida_a_usuari'].sum()\n",
    "# # print(f\"Nombre de piulades dirigides a un usuari concret: {nombre_piulades_dirigides}\")\n",
    "\n",
    "# Patró per detectar usuaris\n",
    "pattern_user = r\"@\\w{4,15}\"\n",
    "\n",
    "# Comptar piulades dirigides a un usuari\n",
    "piulades_dirigides_train = df_tweets_train['tweet_text'].apply(\n",
    "    lambda piulada: bool(re.search(pattern_user, piulada))\n",
    ").sum()\n",
    "\n",
    "piulades_dirigides_test = df_tweets_test['tweet_text'].apply(\n",
    "    lambda piulada: bool(re.search(pattern_user, piulada))\n",
    ").sum()\n",
    "\n",
    "print(f\"En el conjunt d'entrenament hi tenim {piulades_dirigides_train} piulades dirigides a un usuari concret.\")\n",
    "print(f\"En el conjunt de testeigs hi tenim {piulades_dirigides_test} piulades dirigides a un usuari concret.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En el conjunt d'entrenament hi tenim 14413 piulades dirigides a un usuari concret.\n",
      "En el conjunt de testeigs hi tenim 3571 piulades dirigides a un usuari concret.\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Quants *hashtags* ``#`` hi ha a cada categoria de *piulades*?\n",
    "\n",
    "Compteu els ``#`` que vagin seguits d'alguna lletra o nombre ja que, com veiem a la [normativa](https://help.twitter.com/en/using-twitter/how-to-use-hashtags) de X, els ``#`` no poden contenir cap mena de signe de puntuació."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T22:28:11.247721Z",
     "start_time": "2024-12-14T09:37:20.922678Z"
    }
   },
   "source": [
    "# # # # El vostre codi aquí\n",
    "# # # patro_hashtag = r'#\\w+' \n",
    "# # # df['nombre_hashtags'] = df['tweet_text'].apply(lambda text: len(re.findall(patro_hashtag, str(text))))\n",
    "# # # hashtags_per_categoria = df.groupby('cyberbullying_type')['nombre_hashtags'].sum()\n",
    "\n",
    "# # # print(hashtags_per_categoria)\n",
    "\n",
    "# pattern_hashtags = re.compile(r'#([A-Za-z0-9]+)\\b')\n",
    "\n",
    "# # Comptar hashtags en cada piulada\n",
    "# df['nombre_hashtags'] = df['tweet_text'].apply(\n",
    "#     lambda text: len(re.findall(pattern_hashtags, str(text))) if isinstance(text, str) else 0\n",
    "# )\n",
    "\n",
    "# # Comptar hashtags per categoria\n",
    "# hashtags_per_categoria = df.groupby('cyberbullying_type')['nombre_hashtags'].sum()\n",
    "\n",
    "# print(hashtags_per_categoria)\n",
    "\n",
    "\n",
    "pattern_hashtags = re.compile(r'#([A-Za-z0-9]+)\\b')\n",
    "\n",
    "# Comptar hashtags en cada piulada\n",
    "df['nombre_hashtags'] = df['tweet_text'].apply(\n",
    "    lambda text: len(re.findall(pattern_hashtags, str(text))) if isinstance(text, str) else 0\n",
    ")\n",
    "\n",
    "# Comptar hashtags per categoria\n",
    "hashtags_per_categoria = df.groupby('cyberbullying_type')['nombre_hashtags'].sum().reset_index()\n",
    "hashtags_per_categoria.rename(columns={'nombre_hashtags': 'total_hashtags'}, inplace=True)\n",
    "\n",
    "result_train = count_hashtags_categories(df_tweets_train)\n",
    "print(result_train)\n"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'count_hashtags_categories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[77], line 32\u001B[0m\n\u001B[0;32m     29\u001B[0m hashtags_per_categoria \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mgroupby(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcyberbullying_type\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnombre_hashtags\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mreset_index()\n\u001B[0;32m     30\u001B[0m hashtags_per_categoria\u001B[38;5;241m.\u001B[39mrename(columns\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnombre_hashtags\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtotal_hashtags\u001B[39m\u001B[38;5;124m'\u001B[39m}, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 32\u001B[0m result_train \u001B[38;5;241m=\u001B[39m \u001B[43mcount_hashtags_categories\u001B[49m(df_tweets_train)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(result_train)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'count_hashtags_categories' is not defined"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Fes un histograma que representi aquesta distribució?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (8,8))\n",
    "\n",
    "plt.bar(result['cyberbullying_type'], result['hashtags'])\n",
    "plt.title('Distribución del número de hashtags')\n",
    "plt.xlabel('Categoria de Cyberbullying')\n",
    "plt.ylabel('Número de Hashtags')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Comproveu a continuació quantes vegades hi ha un ús invàlid dels ``#``, per exemple ``##`` o ``#@``, on, si no tinguessim en compte la restriccio imposada, hauriem contat 2 ``#`` a ``##`` o 1 ``#`` a ``#@``, quan en realitat son 1 i 0 respectivament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d'usos invàlids de hashtags en el conjunt d'entrenament: 74\n"
     ]
    }
   ],
   "source": [
    "# Patró per detectar hashtags invàlids\n",
    "pattern_invalid = re.compile(r'#(?![A-Za-z0-9])') \n",
    "\n",
    "# Comptar hashtags invàlids en cada piulada\n",
    "df_tweets_train['hashtags_invalids'] = df_tweets_train['tweet_text'].apply(\n",
    "    lambda text: len(re.findall(pattern_invalid, str(text))) if isinstance(text, str) else 0\n",
    ")\n",
    "\n",
    "# Comptar total d'usos invàlids de hashtags\n",
    "invalid_hashtags_train = df_tweets_train['hashtags_invalids'].sum()\n",
    "\n",
    "# Mostrar el resultat\n",
    "print(f\"Nombre total d'usos invàlids de hashtags en el conjunt d'entrenament: {invalid_hashtags_train}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Quantes *piulades* vàlides hi ha de cada categoria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cyberbullying_type  valid_tweets\n",
      "0                  age          6357\n",
      "1            ethnicity          6343\n",
      "2               gender          6377\n",
      "3    not_cyberbullying          6308\n",
      "4  other_cyberbullying          6290\n",
      "5             religion          6472\n"
     ]
    }
   ],
   "source": [
    "pattern_invalid = re.compile(r'#\\W*#|#@')\n",
    "\n",
    "# Comprovar si una piulada és vàlida (no conté hashtags invàlids)\n",
    "df_tweets_train['es_valida'] = df_tweets_train['tweet_text'].apply(\n",
    "    lambda text: len(re.findall(pattern_invalid, str(text))) == 0 if isinstance(text, str) else False\n",
    ")\n",
    "\n",
    "# Comptar piulades vàlides per categoria\n",
    "piulades_valides_per_categoria = df_tweets_train[df_tweets_train['es_valida']].groupby('cyberbullying_type').size().reset_index()\n",
    "\n",
    "# Canviar el nom de la columna per a claredat\n",
    "piulades_valides_per_categoria.rename(columns={0: 'valid_tweets'}, inplace=True)\n",
    "\n",
    "# Mostrar el resultat\n",
    "print(piulades_valides_per_categoria)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Quantes *piulades* de la categoria `not_cyberbullying` és dirigeixen a un usuari?\n",
    "+ Quantes *piulades* de totes les altres categories és dirigeixen a un usuari?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de piulades 'not_cyberbullying' que es dirigeixen a un usuari: 3292\n",
      "Nombre de piulades de les altres categories que es dirigeixen a un usuari: 11201\n"
     ]
    }
   ],
   "source": [
    "# patro_usuari = re.compile(r'@\\w+')\n",
    "\n",
    "# Filtrar piulades de la categoria 'not_cyberbullying' i altres categories\n",
    "piulades_not_cyberbullying = df_tweets_train[df_tweets_train['cyberbullying_type'] == 'not_cyberbullying']\n",
    "piulades_altres_categories = df_tweets_train[df_tweets_train['cyberbullying_type'] != 'not_cyberbullying']\n",
    "\n",
    "# Comptar piulades amb mencions en cada categoria\n",
    "nombre_piulades_not_cyberbullying_amb_mencion = piulades_not_cyberbullying['tweet_text'].apply(\n",
    "    lambda text: bool(re.search(patro_usuari, str(text)))\n",
    ").sum()\n",
    "\n",
    "nombre_piulades_altres_categories_amb_mencion = piulades_altres_categories['tweet_text'].apply(\n",
    "    lambda text: bool(re.search(patro_usuari, str(text)))\n",
    ").sum()\n",
    "\n",
    "# Resultats\n",
    "print(f\"Nombre de piulades 'not_cyberbullying' que es dirigeixen a un usuari: {nombre_piulades_not_cyberbullying_amb_mencion}\")\n",
    "print(f\"Nombre de piulades de les altres categories que es dirigeixen a un usuari: {nombre_piulades_altres_categories_amb_mencion}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Calculeu altres coses que penseu que poden ser rellevants (usuaris més mencionats, *hashtags* més comuns per cada categoria, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T22:28:11.288285500Z",
     "start_time": "2024-12-14T09:12:56.860863Z"
    }
   },
   "source": [
    "# Patró per detectar mencions (@usuari)\n",
    "# patro_usuari = re.compile(r'@(\\w+)')\n",
    "\n",
    "# Extreure totes les mencions i comptar-ne la freqüència\n",
    "totes_les_mencions = df_tweets_train['tweet_text'].dropna().apply(\n",
    "    lambda text: re.findall(patro_usuari, str(text))\n",
    ").explode()\n",
    "\n",
    "usuaris_mes_mencionats = totes_les_mencions.value_counts().head(10)\n",
    "\n",
    "print(\"Usuaris més mencionats:\")\n",
    "print(usuaris_mes_mencionats)\n",
    "\n",
    "\n",
    "\n",
    "# Calcula la longitud de cada piulada\n",
    "df_tweets_train['longitud'] = df_tweets_train['tweet_text'].apply(lambda text: len(str(text)))\n",
    "\n",
    "# Longitud mitjana per categoria\n",
    "longitud_mitjana_per_categoria = df_tweets_train.groupby('cyberbullying_type')['longitud'].mean()\n",
    "\n",
    "print(\"Longitud mitjana de les piulades per categoria:\")\n",
    "print(longitud_mitjana_per_categoria)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'patro_usuari' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Patró per detectar mencions (@usuari)\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# patro_usuari = re.compile(r'@(\\w+)')\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Extreure totes les mencions i comptar-ne la freqüència\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m totes_les_mencions \u001B[38;5;241m=\u001B[39m \u001B[43mdf_tweets_train\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtweet_text\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropna\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mre\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfindall\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatro_usuari\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mexplode()\n\u001B[0;32m      9\u001B[0m usuaris_mes_mencionats \u001B[38;5;241m=\u001B[39m totes_les_mencions\u001B[38;5;241m.\u001B[39mvalue_counts()\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsuaris més mencionats:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[0;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4800\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[1;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[0;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[0;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[0;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[0;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[1;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[1;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[0;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[0;32m   1747\u001B[0m     )\n",
      "File \u001B[1;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[14], line 6\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Patró per detectar mencions (@usuari)\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# patro_usuari = re.compile(r'@(\\w+)')\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Extreure totes les mencions i comptar-ne la freqüència\u001B[39;00m\n\u001B[0;32m      5\u001B[0m totes_les_mencions \u001B[38;5;241m=\u001B[39m df_tweets_train[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet_text\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mdropna()\u001B[38;5;241m.\u001B[39mapply(\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m text: re\u001B[38;5;241m.\u001B[39mfindall(\u001B[43mpatro_usuari\u001B[49m, \u001B[38;5;28mstr\u001B[39m(text))\n\u001B[0;32m      7\u001B[0m )\u001B[38;5;241m.\u001B[39mexplode()\n\u001B[0;32m      9\u001B[0m usuaris_mes_mencionats \u001B[38;5;241m=\u001B[39m totes_les_mencions\u001B[38;5;241m.\u001B[39mvalue_counts()\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsuaris més mencionats:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'patro_usuari' is not defined"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Comptar paraules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:11.277982Z",
     "start_time": "2024-12-14T23:09:11.264056Z"
    }
   },
   "source": [
    "# No modificar aquesta cel·la, s'encarrega de fer el procés més eficient.\n",
    "# Intenteu entendre quà fa aquesta cel·la\n",
    "\n",
    "def memo(f):\n",
    "    class memodict(dict):\n",
    "        def __init__(self, f):\n",
    "            self.f = f\n",
    "        def __call__(self, *args):\n",
    "            return self[args]\n",
    "        def __missing__(self, key):\n",
    "            ret = self[key] = self.f(*key)\n",
    "            return ret\n",
    "    return memodict(f)"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer que haurem d'implementar és la funció ``standardize`` que estandaritzarà les paraules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 2: \n",
    "\n",
    "+ Empleneu la funció següent que, donada una paraula, la estandaritzi les paraules passant tots els caràcters a minúscules.\n",
    "\n",
    "**Observació:** Podeu considerar diverses possibilitats per aquesta funció:\n",
    "* Només canviar les lletres majúscules per minúscules\n",
    "* Eliminar tots els símbols que no siguin @, # i _ (i substituïr-los per un espai)\n",
    "* Eliminar tots els símbols (i substituïr-los per un espai)\n",
    "* etc.\n",
    "\n",
    "El mòdul `re` de Python és especialment útil en aquest punt. \n",
    "\n",
    "Trieu aquella possibilitat que dóna millor resultats a l'exercici 7!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:11.821927Z",
     "start_time": "2024-12-14T23:09:11.813545Z"
    }
   },
   "source": [
    "@memo\n",
    "def standardize(word):\n",
    "    \"\"\"\n",
    "    :param word: paraula a estandaritzar\n",
    "    :return : paraula estandaritzada\n",
    "    \"\"\"\n",
    "    # Comprovació de si 'word' és una cadena\n",
    "    if not isinstance(word, str):\n",
    "        return word\n",
    "\n",
    "    # Convertir a minúscules\n",
    "    word = word.lower()\n",
    "\n",
    "    # Eliminar tots els símbols que no siguin @, # i _\n",
    "    symbolsPattern = re.compile(r'[^a-z0-9@#_]')\n",
    "    word = re.sub(symbolsPattern, ' ', word).strip()\n",
    "\n",
    "    # Substituir múltiples espais per un de sol\n",
    "    word = re.sub(r'\\s+', ' ', word)\n",
    "\n",
    "    return word\n"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:11.965924Z",
     "start_time": "2024-12-14T23:09:11.950251Z"
    }
   },
   "source": [
    "standardize(\"Taller DELS noUS USOS\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taller dels nous usos'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 3: \n",
    "\n",
    "+ Feu una funció ``count_words`` que construeixi un diccionari que contingui totes les paraules que s'han trobat al dataset, tot indicant el total de cops que ha aparegut cadascuna i el nombre de piulades on apareix. \n",
    "\n",
    "El resultat ha de ser un diccionari d'aquest tipus (no necessàriament amb aquest valors):\n",
    "\n",
    "```python\n",
    "{\n",
    "    'memory' : {'n_ocur': 88, 'n_piu': 76},\n",
    "    'best': {'n_ocur': 123, 'n_piu': 65},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:12.021230Z",
     "start_time": "2024-12-14T23:09:12.006154Z"
    }
   },
   "source": [
    "def count_words(df):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame amb les piulades i la informació associada\n",
    "    :return : Diccionari amb el format {word : {n_ocur: valor, n_tweets: valor}, ...}\n",
    "    \"\"\"\n",
    "\n",
    "    dicc = {}\n",
    "    for text in df['tweet_text']:\n",
    "        text_standarized = standardize(text)\n",
    "        words = text_standarized.split()\n",
    "\n",
    "        seen = set()\n",
    "        for word in words:\n",
    "            if word not in dicc:\n",
    "                dicc[word] = {'n_ocur': 0, 'n_piu': 0}\n",
    "            dicc[word]['n_ocur'] += 1\n",
    "            if word not in seen:\n",
    "                dicc[word]['n_piu'] += 1\n",
    "                seen.add(word)\n",
    "    return dicc"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:14.099410Z",
     "start_time": "2024-12-14T23:09:12.138639Z"
    }
   },
   "source": [
    "dicc_text = count_words(df_tweets_train)\n",
    "print (len(dicc_text))\n",
    "print(\"'rape':\",dicc_text['rape'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54055\n",
      "'rape': {'n_ocur': 3570, 'n_piu': 3254}\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comptar paraules per cada categoria de piulada"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:14.567387Z",
     "start_time": "2024-12-14T23:09:14.553484Z"
    }
   },
   "source": [
    "df_tweets_train.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              tweet_text cyberbullying_type\n",
       "33732  fav part of #BellLetsTalk is all the girls who...                age\n",
       "17264  Even men who claimed to be feminist are only p...           religion\n",
       "19441  Oh really. Then kindly shed that who brought p...           religion\n",
       "43902  Dumb nigger shut the fuck up before I hang you...          ethnicity\n",
       "3160               @dale_in_denver Who assumed what now?  not_cyberbullying"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33732</th>\n",
       "      <td>fav part of #BellLetsTalk is all the girls who...</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17264</th>\n",
       "      <td>Even men who claimed to be feminist are only p...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19441</th>\n",
       "      <td>Oh really. Then kindly shed that who brought p...</td>\n",
       "      <td>religion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43902</th>\n",
       "      <td>Dumb nigger shut the fuck up before I hang you...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3160</th>\n",
       "      <td>@dale_in_denver Who assumed what now?</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 4: \n",
    "\n",
    "+ Fent servir la funció que se us dona a continuació (`eachTopic`), apliqueu-la per tal de comptar les paraules que s'han trobat i la seva ocurrència segregant ara per categoria.\n",
    "\n",
    "El resultat ha de ser un diccionari d'aquest tipus (no necessàriament amb aquest valors):\n",
    "\n",
    "```python\n",
    "{\n",
    "    'ethnicity': {\n",
    "        'race' : {'n_ocur': 88, 'n_piu': 76},\n",
    "        'what': {'n_ocur': 123, 'n_piu': 65}\n",
    "        ...\n",
    "    },\n",
    "    ...\n",
    "    'gender': {\n",
    "        'jokes' : {'n_ocur': 18, 'n_piu': 17},\n",
    "        'you': {'n_ocur': 154, 'n_piu': 66}\n",
    "    }\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:14.638994Z",
     "start_time": "2024-12-14T23:09:14.628697Z"
    }
   },
   "source": [
    "def count_words_categories(df):\n",
    "    \"\"\"\n",
    "    Funció que ha de constuir un diccionari que conté la freqüència de les\n",
    "    paraules i el número de piulades on ha aparegut.\n",
    "    Aquesta informació ha de ser dividida per diferents categories de cyberbullying.\n",
    "\n",
    "    :param df: DataFrame amb les piulades i la informació associada\n",
    "    :return : Diccionari amb el format {label : {word : {n_ocur: valor, n_news: valor} } }\n",
    "    \"\"\"\n",
    "    words_topic = {}\n",
    "\n",
    "    def eachTopic(group):\n",
    "        # Count words on this topic and save to dictionary\n",
    "        words_topic[group.name] = count_words(group)\n",
    "\n",
    "    # El vostre codi aquí\n",
    "    df.groupby('cyberbullying_type').apply(eachTopic)\n",
    "\n",
    "    return words_topic\n"
   ],
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:15.612787Z",
     "start_time": "2024-12-14T23:09:14.743140Z"
    }
   },
   "source": [
    "words_categories = count_words_categories(df_tweets_train)\n",
    "print (\"Categories:\", len(words_categories))\n",
    "print (\"La paraula 'muslims' a la categoria 'religion':\", words_categories[\"religion\"]['muslims'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: 6\n",
      "La paraula 'muslims' a la categoria 'religion': {'n_ocur': 1938, 'n_piu': 1691}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_7092\\1496760919.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.groupby('cyberbullying_type').apply(eachTopic)\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Paraules més freqüents a les piulades\n",
    "\n",
    "\n",
    "**El problema de com escollir el vector de carecterístiques**\n",
    "\n",
    "L'elecció de les paraules que formen el vector de característiques és un pas crític. En funció de com de bona sigui aquesta descripció, millor funcionarà el sistema. Tot i que us deixem a vosaltres la política de creació del vector de característiques us donem una pista: una possible estratègia és agafar aquelles paraules que apareixen entre en un 10 i un 50 percent del total de piludades (sense tenir en compte la categoria). És a dir, paraules que no són ni massa freqüents ni massa poc. Podeu experimentar variant aquests valors.\n",
    "\n",
    "Una altra estratègia interessant és eliminar \"stop words\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 5: \n",
    "\n",
    "+ Feu una funció ``topNwords`` que crei un diccionari amb les N paraules més representatives (les que apareixen amb més freqüència) de cadascuna de les categories de *cyberbullying* (Tenint en compte que haureu tret aquelles paraules que apareixen en la majoria de piulades, així com també, les que únicament apareixen en un conjunt molt petit de piulades). Una pista de que aneu ben encaminats es que per cadascuna de les categories de *cyberbullying* obtingueu paraules rellevants per aquesta. Si no es així, vol dir que heu d'incrementar el nombre de paraules a saltar (*skip_top*).\n",
    "\n",
    "El resultat serà un diccionari tipus (no necessàriament amb aquest valors):\n",
    "\n",
    "```python\n",
    "{\n",
    "    'age': ['school', 'high', ...],\n",
    "    ...\n",
    "    'religion': ['muslims', 'christian',...]\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "+ Experimenteu omplint la llista `skip_top` amb aquelles paraules que penseu no tenen significat o relevancia per definir cada categoria. Podeu buscar informació sobre **stop words** a internet i definir varies llistes fins que penseu que obteniu una bona representació de paraules per categoria de `cyberbullying`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-14T23:33:18.415515Z",
     "start_time": "2024-12-14T23:33:18.393820Z"
    }
   },
   "source": [
    "def define_skiptop():\n",
    "    stop_words = [\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\",\n",
    "    \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\",\n",
    "    \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\",\n",
    "    \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\",\n",
    "    \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\",\n",
    "    \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\",\n",
    "    \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\",\n",
    "    \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\",\n",
    "    \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\",\n",
    "    \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
    "    \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n",
    "    \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"can\", \"will\",\n",
    "    \"just\", \"don\", \"should\", \"now\", \"rt\", \"co\", \"u\", \"one\", \"http\", \"https\", \"www\",\n",
    "    \"com\", \"t\", \"s\", \"like\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def topNwords(df, words, N, skip=[]):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame amb les piulades i la informació associada\n",
    "    :param words: diccionari amb les paraules i la seva frequencia\n",
    "    :param N: número de paraules més representatives que volem considerar\n",
    "    :return : Diccionari amb el format {categoria1: llista_top_words_cat_1,  \n",
    "                                        categoria2: llista_top_words_cat_2, ...} \n",
    "    \"\"\"\n",
    "    top_words=dict()\n",
    "\n",
    "    for category in df['cyberbullying_type'].unique():\n",
    "        df_filtered = df[df['cyberbullying_type'] == category]\n",
    "\n",
    "        category_words = {}\n",
    "        for word, info in words[category].items():\n",
    "            if skip is not []:\n",
    "                if word not in skip: category_words[word] = info['n_ocur']\n",
    "            else:\n",
    "                if 0.1 <= (info['n_ocur'] / df_filtered.shape[0]) <= 0.7:\n",
    "                    category_words[word] = info['n_ocur']\n",
    "\n",
    "        sorted_words = sorted(category_words.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        top_words[category] = [word for word, freq in sorted_words[:N]]\n",
    "\n",
    "\n",
    "    return top_words\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:16.101912Z",
     "start_time": "2024-12-14T23:09:16.096221Z"
    }
   },
   "source": [
    "\n"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Vector de Característiques\n",
    "\n",
    "#### EXERCICI 6: \n",
    "\n",
    "Creeu el vector de característiques necessari per a fer l’entrenament del Naïve Bayes amb la funció ``create_features``.\n",
    "\n",
    "El resultat serà un diccionari tipus (no necessàriament amb aquest valors):\n",
    "\n",
    "```python\n",
    "{\n",
    "    0: np.array([0, 1, 1, 0, ...]),\n",
    "    1: np.array([0, 1, 1, 1, ...]),\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:16.118681Z",
     "start_time": "2024-12-14T23:09:16.109428Z"
    }
   },
   "source": [
    "def create_features(df, top_words):\n",
    "    \"\"\"\n",
    "    :params df: DataFrame amb les piulades i la informació associada\n",
    "    :params top_words: ha de ser el diccionari que retorna topNWords\n",
    "    :return : diccionari o pd.Series que conté un np.array per a \n",
    "        cadascuna de les piulades amb el vector de característiques corresponent.\n",
    "    \"\"\"\n",
    "    all_worlds = [word for category, words in top_words.items() for word in words]\n",
    "    all_worlds = set(all_worlds)\n",
    "    n_words = len(all_worlds)\n",
    "    valid_idx = df.index.tolist()\n",
    "    dict_feat_vector = {}\n",
    "    for idx, piulada in enumerate(df['tweet_text']):\n",
    "        vect_piulada = np.zeros(n_words, dtype=int)\n",
    "\n",
    "        piulada_standardized = [standardize(word) for word in piulada.split()]\n",
    "\n",
    "        for pos, top_word in enumerate(all_worlds):\n",
    "            if top_word in piulada_standardized: vect_piulada[pos] = 1\n",
    "\n",
    "        dict_feat_vector[valid_idx[idx]] = vect_piulada\n",
    "\n",
    "\n",
    "    return dict_feat_vector"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:20.073313Z",
     "start_time": "2024-12-14T23:09:16.342148Z"
    }
   },
   "source": [
    "N = 20 # Aquest parametre el podem canviar i fer proves per avaluar quin és el millor valor.\n",
    "words_categories = count_words_categories(df_tweets_train)\n",
    "\n",
    "top_words = topNwords(df_tweets_train, words_categories, N, skip = define_skiptop())\n",
    "dict_feat_vector = create_features(df_tweets_train, top_words)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_7092\\1496760919.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.groupby('cyberbullying_type').apply(eachTopic)\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:20.359674Z",
     "start_time": "2024-12-14T23:09:20.348692Z"
    }
   },
   "source": [
    "len(dict_feat_vector)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38153"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. El classificador Naïve Bayes\n",
    "\n",
    "Un cop tenim una representació necessitem un procés d'aprenentatge que ens permeti passar de la descripció a una categoria. \n",
    "En aquest lliurament farem servir el classificador Naïve Bayes. \n",
    "Aquest classificador forma part de la família de classificadors probabilístics. \n",
    "La sortida d'un classificador probabilístic és un valor de probabilitat donat un exemple per cadascuna de les categories. \n",
    "La decisió final correspon a la categoria amb més probabilitat. \n",
    "\n",
    "\n",
    "Els classificadors probabilistics Bayesians es basen en el teorema de Bayes per realitzar els càlculs per estimar la probabilitat condicionada $p(y|x)$, on $y$ és la categoria i $\\mathbf x$ les característiques de l'exemple.\n",
    "\n",
    "La fòrmula de Bayes és fàcil de deduir. Sabem que:\n",
    "\n",
    "$$ p(\\mathbf x,y) = p(\\mathbf x|y)p(y) = p(y|\\mathbf x)p(\\mathbf x)$$\n",
    "d'on podem extreure que: \n",
    "$$ p(y|\\mathbf x) = \\frac{p(\\mathbf x|y)p(y)}{p(\\mathbf x)}$$\n",
    "\n",
    "\n",
    "La millor categoria serà la que fa màxima la probabilitat $ p(y|\\mathbf x)$ i per comparar aquests valors i saber quin és el màxim no cal calcular $p(\\mathbf x)$ (que és constant). Per tant, considerarem que:\n",
    "$$ p(y|\\mathbf x) \\approx p(y) · p(\\mathbf x|y)$$\n",
    "\n",
    "\n",
    "Les deduccions fins a aquest punt són vàlides per la majoria de classificadors Bayesians. \n",
    "Naïve Bayes es distingeix de la resta perquè imposa una condició encara més restrictiva. \n",
    "Considerem $\\mathbf x=(x_1, \\cdots, x_n)$ un conjunt d'$N$ variables aleatòries (en el nostre cas, les paraules seleccionades). \n",
    "Naïve Bayes assumeix que la probabilitat de la presència d'una paraula en una piulada és independent de la presència d'una altra i per tant podem escriure:\n",
    "$$p(x_1,x_2,...,x_N | y) = p(x_1|y)p(x_2|y)...p(x_N|y)$$\n",
    "\n",
    "\n",
    "Podem interpretar l'anterior equació de la següent forma: La probabilitat de que una piuldad descrita pel vector de característiques (0,1,0,1,1,1) sigui de la classe \"gender\" és proporcional al producte de la probabilitat que la primera paraula del vector no aparegui en les piulades sobre \"gender\" per la probabilitat que la segona paraula sí que hi aparegui, etc.\n",
    "\n",
    "\n",
    "**Estimant les probabilitats marginals condicionades**\n",
    "\n",
    "L'últim pas que ens queda és trobar el valor de les probabilitats condicionades. \n",
    "Farem servir la representació de $0$'s i $1$'s indicant que la paraula no apareix (0) o sí apareix (1) a la piulada. \n",
    "\n",
    "Per trobar el valor de la probabilitat condicionada farem servir una aproximació freqüentista a la probabilitat. \n",
    "Això vol dir que calcularem la freqüència d'aparició de cada paraula per a cada categoria. \n",
    "Aquest càlcul es fa dividint el nombre de piulades de la categoria en que apareix la paraula pel nombre total de piulades d'aquella categoria. \n",
    "\n",
    "En general:\n",
    "$$p(x = \\text{\"school\"} | y = C)= \\frac{A}{B} $$\n",
    "on $A$ és el número de piulades de la categoria $C$ on hi apareix la paraula 'school' i $B$ és el número total de piulades de la categoria $C$.\n",
    "\n",
    "\n",
    "#### Punts delicats a tenir en compte.\n",
    "\n",
    "**El problema de la probabilitat 0**\n",
    "\n",
    "Si us hi fixeu bé, la probabilitat pot ser 0!!  Això vol dir, que si en una piulada no hi apareix una paraula, no pot ser classificada com cap tipus de *cyber bullying* (la presència del 0 al producte fa que el resultat sigui 0).\n",
    "\n",
    "No sembla raonable que s'assigni o no en aquesta categoria segons si en la piulada hi apareix o no una única paraula. \n",
    "Per tant, el que s'acostuma a fer és donar una baixa probabilitat en comptes de zero. \n",
    "\n",
    "Una de les possibles solucions es fer servir la correcció de Laplace. Seguint l'exemple anterior la correcció de Laplace és:\n",
    "\n",
    "$$p(x= \\text{\"school\"} | y = 'C' ) = \\frac{A+1}{B+M}$$ \n",
    "\n",
    "on $M$ és el nombre de categories.\n",
    "\n",
    "**El problema de l'\"underflow\"**\n",
    "\n",
    "La valor que hem de calcular en el Naive Bayes és el resultat d'un producte. \n",
    "El nombre de caractéristiques del vector és el nombre de termes del producte. \n",
    "Aquests nombres són iguals o menors a 1 i n'hi ha molts, si els multipliquem entre ells el resultat serà massa petit per a representar-lo en un nombre de punt flotant i el càlcul acabarà sent reduït a zero. \n",
    "\n",
    "Per solucionar aquest problema en comptes d'operar fent multiplicacions, se sol passar a l'escala logarítmica i allà operar fent servir sumes en comptes de multiplicacions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 7: \n",
    "\n",
    "+ Implementeu la funció d'aprenentatge del classificador Naïve Bayes (funció ``naive_bayes_learn()``) que retorni un diccionari amb estructura `{categoria: [P0, ..., PN]}` on la llista representa la probabilitat\n",
    "marginal condicionada de cada paraula del vector de característiques per la categoria corresponent. \n",
    "\n",
    "+ Implementeu la funció ``naive_bayes`` que implementa el classificador. Noteu que aquesta funció está guiada i només haureu d'emplenar els espais on hem posat tres punts suspensius \"#···\".  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:20.861206Z",
     "start_time": "2024-12-14T23:09:20.852776Z"
    }
   },
   "source": [
    "def naive_bayes_learn(df, feats):\n",
    "    \"\"\"\n",
    "    :params df: DataFrame amb les piulades i la informació associada\n",
    "    :params feats: vector de característiques de cada piulada\n",
    "    :return : probabilitats marginals condicionades\n",
    "    \"\"\"\n",
    "    probs = {}\n",
    "    cyberbullying_type  = df['cyberbullying_type'].unique()\n",
    "    M = len(cyberbullying_type)\n",
    "\n",
    "    for category in cyberbullying_type:\n",
    "        # Filtrar tweets de la categoría actual\n",
    "        df_filtered = df[df['cyberbullying_type'] == category]\n",
    "\n",
    "        category_idx = df_filtered.index.tolist()\n",
    "\n",
    "        B = len(category_idx)\n",
    "        vector_length = len(next(iter(feats.values())))\n",
    "        vector = np.zeros(vector_length, dtype=float)\n",
    "\n",
    "        # Iterate over valid indices only\n",
    "        valid_indices = [idx for idx in category_idx if idx in feats]\n",
    "\n",
    "        for idx in valid_indices:\n",
    "            vector += feats[idx]\n",
    "\n",
    "        for idx, A in enumerate(vector):\n",
    "            vector[idx] = (A + 1) / (B + M)\n",
    "\n",
    "        probs[category] = vector\n",
    "\n",
    "    return probs\n"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:20.875572Z",
     "start_time": "2024-12-14T23:09:20.866460Z"
    }
   },
   "source": [
    "import sys\n",
    "from IPython import embed\n",
    "def naive_bayes(df_train, feat_train, feat_test=None, df_test=None):\n",
    "    \"\"\"\n",
    "    Funció que implementa el clasificador Naive_Bayes.\n",
    "\n",
    "    Si df_test no és None, ha de calcular l'encert sobre les dades de test. És a dir,\n",
    "    després de classificar feat_test ha de comparar la classificació amb la classe\n",
    "    real i dir (print) quin percentatge d'encert ha obtingut.\n",
    "\n",
    "    :param df_train: DataFrame amb les piulades que s'utilitzaran per l'entrenament\n",
    "    :param feat_train: Diccionari amb els vectors de caracteristiques de cada tweet de l'entrenament\n",
    "    :param feat_test: Diccionari amb els vectors de caracteristiques de cada tweet de test\n",
    "    :param df_test: DataFrame amb les piulades que s'utilitzaran pel test\n",
    "\n",
    "    :return : Una serie on l'index correspon amb els indexos de df_test i els valors són la\n",
    "              classificació retornada per Naive Bayes\n",
    "    \"\"\"\n",
    "    probs = naive_bayes_learn(df_train, feat_train)\n",
    "    #p_of_cat = count_words_categories(df_train)\n",
    "    #p_total = len(p_of_cat.keys())\n",
    "\n",
    "    p_total = len(df_train)\n",
    "    p_of_cat = df_train['cyberbullying_type'].value_counts() / p_total\n",
    "\n",
    "    def eachFeats(row):\n",
    "        id, feat = row\n",
    "        p_max = -float('inf')\n",
    "        feat = np.array(feat)\n",
    "        p_cat = None\n",
    "\n",
    "        for category in probs:\n",
    "            # Speed up by using numpy\n",
    "            # inv is the inverse of features, 0 where 1 and 1 where 0\n",
    "            # ...\n",
    "\n",
    "            # Probs * feats is the probability of being there, while\n",
    "            # inv - inv * feat = 1 - (0, 1, 0... inverses) * probs, probability of not being there\n",
    "            # ...\n",
    "\n",
    "            # Sum of logs [vs] underflow caused by mul of probs\n",
    "            # ...\n",
    "\n",
    "            # Take the max, do it now to avoid extra-loops\n",
    "            # ...\n",
    "\n",
    "            prob_category = np.log(p_of_cat[category])\n",
    "            prob_category += np.sum(\n",
    "                np.log(probs[category]) * feat + np.log(1 - probs[category]) * (1 - feat)\n",
    "            )\n",
    "\n",
    "            if prob_category > p_max:\n",
    "                p_max = prob_category\n",
    "                p_cat = category\n",
    "\n",
    "        return id, p_cat\n",
    "\n",
    "    data = map(eachFeats, feat_test.items())\n",
    "    data = pd.Series(dict(data)).reindex(df_test.index)\n",
    "    correct = data == df_test['cyberbullying_type']\n",
    "    print(\"Accuracy: {}\".format(correct.sum() / correct.size))\n",
    "\n",
    "    return correct.sum() / correct.size"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:32:50.514416Z",
     "start_time": "2024-12-14T23:32:42.522445Z"
    }
   },
   "source": [
    "N = 20 # Aquest parametre el podeu canviar i fer proves per avaluar quin és el millor valor.\n",
    "\n",
    "words_topics = count_words_categories(df_tweets_train)\n",
    "top_words = topNwords(df_tweets_train, words_topics, N, skip = [])\n",
    "\n",
    "feat_train = create_features(df_tweets_train, top_words)\n",
    "feat_test = create_features(df_tweets_test, top_words)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_7092\\1496760919.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.groupby('cyberbullying_type').apply(eachTopic)\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T23:09:24.098525Z",
     "start_time": "2024-12-14T23:09:23.108675Z"
    }
   },
   "source": "accuracy = naive_bayes(df_tweets_train, feat_train, feat_test, df_tweets_test)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6717685291959324\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haurieu d'obtenir una precisió del 67-70%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 8: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El possible procediment per tal d'aconseguir una major precisió seria el següent:\n",
    "+ Es poden implementar diverses maneres d'escollir quines *stopwords* volem eliminar en les piuldades per a que no formin part dels vectors de característiques.\n",
    "+ Avaluar quin conjunt d'*stopwords* retorna una precisió major per a N=40. \n",
    "+ Un cop tinguem el vector d'*stopwords* amb millors resultats, el testejarem per a diferents $N$'s per a veure quina és el nombre de *stopwords* òptim.\n",
    "\n",
    "Feu una cerca a Intenet per trobar més estratègies i intenteu millorar l'*accuracy* que heu acosneguit fins ara.\n",
    "\n",
    "Amb això podeu arribar a precisions superiors al 80%."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T09:07:52.391609Z",
     "start_time": "2024-12-15T09:07:52.284382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words_ingles = set(stopwords.words('english'))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T09:35:49.724319Z",
     "start_time": "2024-12-15T09:35:47.592773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N = 500\n",
    "\n",
    "words_topics = count_words_categories(df_tweets_train)\n",
    "\n",
    "top_words = topNwords(df_tweets_train, words_topics, N, skip = ['im'])\n",
    "# Crear un diccionario para contar las palabras\n",
    "# 2. Combinar todas las palabras de las categorías\n",
    "all_words = count_words_categories(df_tweets_train)\n",
    "\n",
    "# 3. Identificar palabras repetitivas (aparecen en varias categorías)\n",
    "repeated_words = {\n",
    "    word for word in set(all_words)\n",
    "    if sum(word in words for words in top_words.values()) > 3\n",
    "}\n",
    "stop_words = repeated_words.union(stop_words_ingles)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_7092\\1496760919.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.groupby('cyberbullying_type').apply(eachTopic)\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_7092\\1496760919.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df.groupby('cyberbullying_type').apply(eachTopic)\n"
     ]
    }
   ],
   "execution_count": 170
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T09:36:17.550794Z",
     "start_time": "2024-12-15T09:36:11.492100Z"
    }
   },
   "source": [
    "N = 80\n",
    "top_words = topNwords(df_tweets_train, words_topics, N, skip = stop_words)\n",
    "feat_train = create_features(df_tweets_train, top_words)\n",
    "feat_test = create_features(df_tweets_test, top_words)\n",
    "accuracy = naive_bayes(df_tweets_train, feat_train, feat_test, df_tweets_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7962050529405598\n"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T09:32:35.415895Z",
     "start_time": "2024-12-15T09:32:28.277715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N = 120\n",
    "top_words = topNwords(df_tweets_train, words_topics, N, skip = stop_words)\n",
    "feat_train = create_features(df_tweets_train, top_words)\n",
    "feat_test = create_features(df_tweets_test, top_words)\n",
    "accuracy = naive_bayes(df_tweets_train, feat_train, feat_test, df_tweets_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8023901876506971\n"
     ]
    }
   ],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T09:28:26.922359Z",
     "start_time": "2024-12-15T09:28:18.888537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N = 160\n",
    "top_words = topNwords(df_tweets_train, words_topics, N, skip = stop_words)\n",
    "feat_train = create_features(df_tweets_train, top_words)\n",
    "feat_test = create_features(df_tweets_test, top_words)\n",
    "accuracy = naive_bayes(df_tweets_train, feat_train, feat_test, df_tweets_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8034385155676695\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T09:34:40.546836Z",
     "start_time": "2024-12-15T09:34:12.858399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N = 900\n",
    "top_words = topNwords(df_tweets_train, words_topics, N, skip = stop_words)\n",
    "feat_train = create_features(df_tweets_train, top_words)\n",
    "feat_test = create_features(df_tweets_test, top_words)\n",
    "accuracy = naive_bayes(df_tweets_train, feat_train, feat_test, df_tweets_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8173812768634029\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T09:31:30.652157Z",
     "start_time": "2024-12-15T09:30:50.879099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N = 1200\n",
    "top_words = topNwords(df_tweets_train, words_topics, N, skip = stop_words)\n",
    "feat_train = create_features(df_tweets_train, top_words)\n",
    "feat_test = create_features(df_tweets_test, top_words)\n",
    "accuracy = naive_bayes(df_tweets_train, feat_train, feat_test, df_tweets_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8180102736135864\n"
     ]
    }
   ],
   "execution_count": 153
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
