{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes i Classificació\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En aquest tercer lliurament es programarà un classificador, que donada una *piulada*, la categoritzarà en una de les possibles classes. En aquesta ocasió, implementareu un classificador amb *piulades* relacionades amb el *cyber bullying*.\n",
    "\n",
    "Recordeu que l'objectiu principal d'aquesta pràctica és desenvolupar un classificador amb una *accuracy* alta. \n",
    "\n",
    "\n",
    "**Què s’ha de fer?**\n",
    "\n",
    "Volem classificar *piulades* segons a quin tipus de *cyber bullying* pertanyen. Així doncs, a partir de tots les *piulades* que tenim, crearem un vector de característiques que ens descrigui cadascuna. A continuació desenvoluparem un classificador probabilístic del tipus **Naive Bayes** que ens permeti identificar a quina classe de *cyber bullying* pertany una *piulada* donada, segons les característiques disenyades.\n",
    "\n",
    "\n",
    "**Quina és la idea del sistema de classificació que s’ha de desenvolupar?**\n",
    "\n",
    "El classificador és un concepte de l'aprenentatge automàtic supervisat. L'objectiu del classificador és donat un vector de característiques que descriuen els objectes que es volen classificar indicar a quina categoria o classe pertanyen d'entre un conjunt predeterminat. \n",
    "\n",
    "El procés de classificació consta de dues parts: \n",
    "\n",
    "+ el procés d'aprenentatge i \n",
    "+ el procés d'explotació o testeig. \n",
    "\n",
    "El procés d'aprenentatge rep exemples de parelles $(x,y)$ on $x$ són les característiques, usualment representades per nombres reals, i $y$ és la categoria a la que pertanyen. \n",
    "Aquest conjunt se'l coneix com a conjunt d'entrenament i ens servirà per trobar una funció $\\hat{y}=f(x)$ que donada una $x$ aconsegueixi que $\\hat{y}$ sigui semblant $y$. \n",
    "\n",
    "Per altra banda el procés de testeig aplica la funció $f(x)$ apresa a l'entrenament a a dades no presents en el conjunt d'aprenentatge per avaluar el classificador.\n",
    "\n",
    "**Classificació i llenguatge natural**\n",
    "\n",
    "La descripció dels exemples en característiques és el punt més crític de tot sistema d'aprenentatge automàtic. \n",
    "Una de les representacions més simples per tal de descriure un text és la representació [\"bag-of-words\"](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "Aquesta representació converteix un text en un vector de $N$ paraules. \n",
    "Primer haurem de seleccionar el conjunt d'$N$ paraules que volem fer servir. Després, per cada paraula comptar quants cops apareix en el text. \n",
    "\n",
    "Una versió alternativa i més simple d'aquest procés pot ser simplement indicar al vector si una determinada paraula apareix o no en el text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abans de començar\n",
    "\n",
    "\n",
    "**\\+ Durant la pràctica, solament es podran fer servir les següents llibreries**:\n",
    "\n",
    "`Pandas, Numpy, Re` i `NLTK`\n",
    "\n",
    "Python té un paquet integrat anomenat [\"re\"](https://www.w3schools.com/python/python_regex.asp), que es pot utilitzar per treballar amb expressions regulars. És molt aconsellable fer servir aquest paquet per processar les paraules.\n",
    "\n",
    "**\\+ No es poden modificar les definicions de les funcions donades, ni canviar els noms de les variables i paràmetres ja donats**\n",
    "\n",
    "Això no implica però que els hàgiu de fer servir. És a dir, que la funció tingui un paràmetre anomenat `df` no implica que l'hàgiu de fer servir, si no ho trobeu convenient.\n",
    "\n",
    "**\\+ En les funcions, s'especifica que serà i de quin tipus cada un dels paràmetres, cal respectar-ho**\n",
    "\n",
    "Per exemple (ho posarà en el pydoc de la funció), `df` sempre serà indicatiu del `Pandas.DataFrame` de les dades. Durant els testos, els paràmetres (i específicament `df`) no contindran les mateixes dades que en aquest notebook, si bé si seran del mateix tipus! Per tant, no us refieu de què tinguin, per exemple, el mateix nombre de files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les dades\n",
    "\n",
    "El 15 d'Abril de 2020, UNICEF va llançar una alarma com a resposta de l'augment de risc de *cyber bullying* durant la pandèmia COVID-19. \n",
    "\n",
    "Les estadístiques són prou alarmants: un 36.5% dels estudiants de l'escola fins a l'institut s'han sentit víctimes del *cyber bullying* i un 87% n'han estat testimonis, amb efectes que van des d'una disminució de resultats acadèmics fins a pensaments suïcides.\n",
    "\n",
    "Amb l'objectiu d'ajudar a l'analisis de la situació, s'ha construit un dataset que conté més de 47000 *piulades* etiquetades d'acord amb la classe de *cyber bullying* que s'està donant:\n",
    "\n",
    "1. `age`;\n",
    "2. `ethnicity`;\n",
    "3. `gender`;\n",
    "4. `religion`;\n",
    "5. `other type of cyberbullying`;\n",
    "6. `not cyberbullying`\n",
    "\n",
    "Les dades han estat balancejades per tal de contenir aproximadament 8000 mostres de cada classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar les dades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura de les dades"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "df = pd.read_csv('./cyberbullying_tweets.csv')\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df['cyberbullying_type'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparació del dataset\n",
    "\n",
    "Dividim les *piulades* en un conjunt d'entrenament, *train*, i en un conjunt de validació, *test*, per tal de poder entrenar i validar el nostre model de classificació."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_tweets_train, df_tweets_test = train_test_split(df, test_size=0.2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com les dades estaven balancejades originalment, podem observar que la distribució de cadascuna de les classes es manté:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_tweets_train['cyberbullying_type'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_tweets_test['cyberbullying_type'].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementació\n",
    "\n",
    "Dividirem el notebook en 3 seccions que es complementen una a l'altra:\n",
    "\n",
    "1. Anàlisi exploratòria de les dades: Informació bàsica sobre les *piulades*.\n",
    "2. Processament de les dades: Creació d'un vector de característiques a partir de les *piulades*.\n",
    "3. Desenvolupament d'un classificador *Naive Bayes*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Anàlisis de dades\n",
    "\n",
    "El primer que haurem de fer és analitzar les dades per veure una mica com són. El que us proposem és fer una sèrie de visualitzacions per observar les dades, com ara:\n",
    "\n",
    "* el nombre de *piulades* que s'estan dirigint a una persona en concret\n",
    "* el nombre de  *hashtags* que hi ha a cada categoria de *piulades*\n",
    "* el nombre de  *piulades* que hi ha de cada categoria de *piulades*\n",
    "* el nombre de  *piulades* de la categoria `not_cyberbullying` que es dirigeixen a un usuari vs totes les altres categories\n",
    "* altres coses que penseu que poden ser rellevants."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "df_tweets_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 1: \n",
    "\n",
    "* Quantes *piulades* estan dirigides a un usuari concret? Diem que una piulada va dirigida a una persona en concret si contenen el patró ``@usuari``. \n",
    "\n",
    "[Restriccions](https://help.twitter.com/en/managing-your-account/change-twitter-handle) que imposa X sobre el patró:\n",
    "+ Nomes són valids aquells patrons amb caràcters alfanumèrics ``[A-z 0-9]`` i el caràcter ``_``\n",
    "+ Els patrons han de començar per ``@`` i han de tenir més de 4 i menys de 16 caràcters.\n",
    "\n",
    "En aquest exercici és extremadament útil l'ús del mòdul ``re``."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Definim el patró per detectar usuaris\n",
    "patro_usuari = re.compile(r'@[A-Za-z0-9_]{4,15}\\b')\n",
    "\n",
    "def comptar_piulades_amb_usuari(df):\n",
    "    # Filtra les piulades que compleixen el patró\n",
    "    piulades_amb_usuari = df['tweet_text'].apply(lambda text: bool(patro_usuari.search(text)))\n",
    "    \n",
    "    # Retorna el total de coincidències\n",
    "    return piulades_amb_usuari.sum()\n",
    "\n",
    "\n",
    "nombre_piulades_amb_usuari = comptar_piulades_amb_usuari(df_tweets_train)\n",
    "# Mostrem el resultat per al conjunt d'entrenament i testeig\n",
    "print(f\"Nombre de piulades dirigides a un usuari concret: {nombre_piulades_amb_usuari}\")\n",
    "print(f\"En el Conjunt de testeigs hi tenim {comptar_piulades_amb_usuari(df_tweets_test)} piulades dirigides a un usuari concret\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Quants *hashtags* ``#`` hi ha a cada categoria de *piulades*?\n",
    "\n",
    "Compteu els ``#`` que vagin seguits d'alguna lletra o nombre ja que, com veiem a la [normativa](https://help.twitter.com/en/using-twitter/how-to-use-hashtags) de X, els ``#`` no poden contenir cap mena de signe de puntuació."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Definim un patró d'expressió regular per identificar hashtags\n",
    "patro_hashtag = re.compile(r'#\\w+')\n",
    "\n",
    "def comptar_hashtags_per_categoria(df):\n",
    "\n",
    "    # Crear una nova columna amb el nombre de hashtags per cada piulada\n",
    "    df['num_hashtags'] = df['tweet_text'].apply(lambda text: len(patro_hashtag.findall(text)))\n",
    "    \n",
    "    # Agrupar per categoria i sumar els hashtags\n",
    "    resultat = df.groupby('cyberbullying_type')['num_hashtags'].sum().reset_index()\n",
    "    resultat.columns = ['cyberbullying_type', 'total_hashtags']\n",
    "    \n",
    "    return resultat\n",
    "\n",
    "hashtags_per_categoria = comptar_hashtags_per_categoria(df_tweets_train)\n",
    "# Imprimim el resultant que mostra el nombre total de hashtags per cada categoria\n",
    "print(hashtags_per_categoria)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Fes un histograma que representi aquesta distribució?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Crear el gràfic de barres\n",
    "plt.bar(hashtags_per_categoria['cyberbullying_type'], hashtags_per_categoria['total_hashtags'], color='pink', edgecolor='black')\n",
    "\n",
    "# Afegir títol i etiquetes\n",
    "plt.title('Distribució del nombre de hashtags per categoria', fontsize=16)\n",
    "plt.xlabel('Categoria de Cyberbullying', fontsize=12)\n",
    "plt.ylabel('Nombre de Hashtags', fontsize=12)\n",
    "\n",
    "# Mostrar el gràfic\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Comproveu a continuació quantes vegades hi ha un ús invàlid dels ``#``, per exemple ``##`` o ``#@``, on, si no tinguessim en compte la restriccio imposada, hauriem contat 2 ``#`` a ``##`` o 1 ``#`` a ``#@``, quan en realitat son 1 i 0 respectivament."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def comptar_usos_incorrectes(df, col_text):\n",
    "    \n",
    "    # Patró per identificar usos incorrectes\n",
    "    pattern_invalid = re.compile(r'##|#[@\\W]')\n",
    "    \n",
    "    # Comptar usos incorrectes\n",
    "    usos_incorrectes = df[col_text].str.count(pattern_invalid).sum()\n",
    "    return usos_incorrectes\n",
    "\n",
    "usos_incorrectes = comptar_usos_incorrectes(df_tweets_train, 'tweet_text')\n",
    "# Imprimim el resultat que mostra el nombre total d'usos incorrectes dels hashtags\n",
    "print(f\"Nombre total d'usos incorrectes dels hashtags: {usos_incorrectes}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Quantes *piulades* vàlides hi ha de cada categoria?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def comptar_categories_valides(df):\n",
    "    # Patrons per detectar hashtags vàlids i invàlids\n",
    "    patro_invalid = re.compile(r'#\\W*#|#@')  # Hashtags mal formats\n",
    "    patro_valid = re.compile(r'(?<!#)(#(?!\\W*#|@)[A-Za-z0-9]+)\\b')  # Hashtags correctes\n",
    "\n",
    "    # Marquem si cada piulada és vàlida\n",
    "    df['es_valida'] = df['tweet_text'].apply(\n",
    "        lambda piulada: len(re.findall(patro_invalid, str(piulada))) == 0 if isinstance(piulada, str) else False\n",
    "    )\n",
    "\n",
    "    # Agrupem per categoria i comptem piulades vàlides\n",
    "    piulades_valides_per_categoria = df.groupby('cyberbullying_type')['es_valida'].sum().reset_index()\n",
    "\n",
    "    # Renomenem la columna resultant\n",
    "    piulades_valides_per_categoria.rename(columns={'es_valida': 'piulades_valides'}, inplace=True)\n",
    "    return piulades_valides_per_categoria\n",
    "\n",
    "resultat = comptar_categories_valides(df_tweets_train)\n",
    "print(resultat)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Quantes *piulades* de la categoria `not_cyberbullying` és dirigeixen a un usuari?\n",
    "+ Quantes *piulades* de totes les altres categories és dirigeixen a un usuari?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def comptar_piulades_dirigides_per_categoria(df):\n",
    "    # Patró per detectar mencions d'usuaris\n",
    "    patro_usuari = re.compile(r'@\\w+')\n",
    "\n",
    "    # Comptem les piulades dirigides a usuaris a la categoria 'not_cyberbullying'\n",
    "    piulades_no_ciberassetjament = df[df['cyberbullying_type'] == 'not_cyberbullying']\n",
    "    compte_usuaris_no_ciberassetjament = piulades_no_ciberassetjament['tweet_text'].apply(\n",
    "        lambda text: bool(re.findall(patro_usuari, str(text))) if isinstance(text, str) else False\n",
    "    ).sum()\n",
    "\n",
    "    # Comptem les piulades dirigides a usuaris a les altres categories\n",
    "    altres_categories = df[df['cyberbullying_type'] != 'not_cyberbullying']\n",
    "    compte_usuaris_altres_categories = altres_categories['tweet_text'].apply(\n",
    "        lambda text: bool(re.findall(patro_usuari, str(text))) if isinstance(text, str) else False\n",
    "    ).sum()\n",
    "\n",
    "    return compte_usuaris_no_ciberassetjament, compte_usuaris_altres_categories\n",
    "\n",
    "compte_no_ciberassetjament, compte_altres_categories = comptar_piulades_dirigides_per_categoria(df_tweets_train)\n",
    "\n",
    "# Mostrar el resultat\n",
    "print(f'Piulades de la categoria \"not_cyberbullying\" dirigides a un usuari: {compte_no_ciberassetjament}')\n",
    "print(f'Piulades de totes les altres categories dirigides a un usuari: {compte_altres_categories}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Calculeu altres coses que penseu que poden ser rellevants (usuaris més mencionats, *hashtags* més comuns per cada categoria, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def analitzar_piulades(df):\n",
    "    # Patró per detectar usuaris mencionats\n",
    "    patro_usuari = r'@[A-Za-z0-9_]{4,15}'\n",
    "\n",
    "    # Extreure mencions d'usuaris i comptar la seva freqüència\n",
    "    mencions = df['tweet_text'].dropna().apply(\n",
    "        lambda text: re.findall(patro_usuari, str(text))\n",
    "    ).explode()\n",
    "    usuaris_mes_mencionats = mencions.value_counts().head(10)\n",
    "\n",
    "    # Calcula la longitud de cada piulada\n",
    "    df['longitud'] = df['tweet_text'].apply(lambda text: len(str(text)))\n",
    "\n",
    "    # Longitud mitjana de les piulades per categoria\n",
    "    longitud_mitjana_per_categoria = df.groupby('cyberbullying_type')['longitud'].mean()\n",
    "\n",
    "    # Mostrar resultats\n",
    "    print(\"Usuaris més mencionats:\")\n",
    "    print(usuaris_mes_mencionats)\n",
    "    print(\"\\nLongitud mitjana de les piulades per categoria:\")\n",
    "    print(longitud_mitjana_per_categoria)\n",
    "\n",
    "analitzar_piulades(df_tweets_train)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Comptar paraules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# No modificar aquesta cel·la, s'encarrega de fer el procés més eficient.\n",
    "# Intenteu entendre quà fa aquesta cel·la\n",
    "\n",
    "def memo(f):\n",
    "    class memodict(dict):\n",
    "        def __init__(self, f):\n",
    "            self.f = f\n",
    "        def __call__(self, *args):\n",
    "            return self[args]\n",
    "        def __missing__(self, key):\n",
    "            ret = self[key] = self.f(*key)\n",
    "            return ret\n",
    "    return memodict(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer que haurem d'implementar és la funció ``standardize`` que estandaritzarà les paraules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 2: \n",
    "\n",
    "+ Empleneu la funció següent que, donada una paraula, la estandaritzi les paraules passant tots els caràcters a minúscules.\n",
    "\n",
    "**Observació:** Podeu considerar diverses possibilitats per aquesta funció:\n",
    "* Només canviar les lletres majúscules per minúscules\n",
    "* Eliminar tots els símbols que no siguin @, # i _ (i substituïr-los per un espai)\n",
    "* Eliminar tots els símbols (i substituïr-los per un espai)\n",
    "* etc.\n",
    "\n",
    "El mòdul `re` de Python és especialment útil en aquest punt. \n",
    "\n",
    "Trieu aquella possibilitat que dóna millor resultats a l'exercici 7!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-12-15T15:02:20.986544Z",
     "start_time": "2024-12-15T15:02:20.943927Z"
    }
   },
   "source": [
    "@memo\n",
    "def standardize(word):\n",
    "    \"\"\"\n",
    "    :param word: paraula a estandaritzar\n",
    "    :return : paraula estandaritzada\n",
    "    \"\"\"\n",
    "    # Comprovació de si 'word' és una cadena\n",
    "    if not isinstance(word, str):\n",
    "        return word\n",
    "\n",
    "    # Convertir a minúscules\n",
    "    word = word.lower()\n",
    "\n",
    "    # Eliminar tots els símbols que no siguin @, # i _\n",
    "    symbolsPattern = re.compile(r'[^a-z0-9@#_]')\n",
    "    word = re.sub(symbolsPattern, ' ', word).strip()\n",
    "\n",
    "    # Substituir múltiples espais per un de sol\n",
    "    word = re.sub(r'\\s+', ' ', word)\n",
    "    if len(word) < 2: return \"\"\n",
    "\n",
    "    return word\n"
   ],
   "outputs": [],
   "execution_count": 425
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "standardize(\"Taller DELS noUS USOS\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 3: \n",
    "\n",
    "+ Feu una funció ``count_words`` que construeixi un diccionari que contingui totes les paraules que s'han trobat al dataset, tot indicant el total de cops que ha aparegut cadascuna i el nombre de piulades on apareix. \n",
    "\n",
    "El resultat ha de ser un diccionari d'aquest tipus (no necessàriament amb aquest valors):\n",
    "\n",
    "```python\n",
    "{\n",
    "    'memory' : {'n_ocur': 88, 'n_piu': 76},\n",
    "    'best': {'n_ocur': 123, 'n_piu': 65},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "source": [
    "def count_words(df):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame amb les piulades i la informació associada\n",
    "    :return : Diccionari amb el format {word : {n_ocur: valor, n_tweets: valor}, ...}\n",
    "    \"\"\"\n",
    "\n",
    "    dicc = {}\n",
    "    # Iterem per cada piulada del DataFrame\n",
    "    for text in df['tweet_text']:\n",
    "        text_standarized = standardize(text)\n",
    "        words = text_standarized.split()\n",
    "\n",
    "        seen = set()\n",
    "        # Iterem per cada paraula al text\n",
    "        for word in words:\n",
    "            # Si la paraula no és al diccionari, inicialitzem la seva entrada\n",
    "            if word not in dicc:\n",
    "                dicc[word] = {'n_ocur': 0, 'n_piu': 0}\n",
    "                \n",
    "            # Incrementem el comptador del nombre d'ocurrències\n",
    "            dicc[word]['n_ocur'] += 1\n",
    "            \n",
    "            # Si la paraula encara no s'ha comptat per aquesta piulada, actualitzem el comptador de piulades\n",
    "            if word not in seen:\n",
    "                dicc[word]['n_piu'] += 1\n",
    "                seen.add(word) # Marquem la paraula com a vista per aquesta piulada\n",
    "    return dicc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dicc_text = count_words(df_tweets_train)\n",
    "print (len(dicc_text))\n",
    "print(\"'rape':\",dicc_text['rape'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comptar paraules per cada categoria de piulada"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_tweets_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 4: \n",
    "\n",
    "+ Fent servir la funció que se us dona a continuació (`eachTopic`), apliqueu-la per tal de comptar les paraules que s'han trobat i la seva ocurrència segregant ara per categoria.\n",
    "\n",
    "El resultat ha de ser un diccionari d'aquest tipus (no necessàriament amb aquest valors):\n",
    "\n",
    "```python\n",
    "{\n",
    "    'ethnicity': {\n",
    "        'race' : {'n_ocur': 88, 'n_piu': 76},\n",
    "        'what': {'n_ocur': 123, 'n_piu': 65}\n",
    "        ...\n",
    "    },\n",
    "    ...\n",
    "    'gender': {\n",
    "        'jokes' : {'n_ocur': 18, 'n_piu': 17},\n",
    "        'you': {'n_ocur': 154, 'n_piu': 66}\n",
    "    }\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "def count_words_categories(df):\n",
    "    \"\"\"\n",
    "    Funció que ha de constuir un diccionari que conté la freqüència de les\n",
    "    paraules i el número de piulades on ha aparegut.\n",
    "    Aquesta informació ha de ser dividida per diferents categories de cyberbullying.\n",
    "\n",
    "    :param df: DataFrame amb les piulades i la informació associada\n",
    "    :return : Diccionari amb el format {label : {word : {n_ocur: valor, n_news: valor} } }\n",
    "    \"\"\"\n",
    "    words_topic = {}\n",
    "\n",
    "    def eachTopic(group):\n",
    "        # Count words on this topic and save to dictionary\n",
    "        words_topic[group.name] = count_words(group)\n",
    "\n",
    "    df.groupby('cyberbullying_type').apply(eachTopic)\n",
    "\n",
    "    return words_topic\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "words_categories = count_words_categories(df_tweets_train)\n",
    "print (\"Categories:\", len(words_categories))\n",
    "print (\"La paraula 'muslims' a la categoria 'religion':\", words_categories[\"religion\"]['muslims'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Paraules més freqüents a les piulades\n",
    "\n",
    "\n",
    "**El problema de com escollir el vector de carecterístiques**\n",
    "\n",
    "L'elecció de les paraules que formen el vector de característiques és un pas crític. En funció de com de bona sigui aquesta descripció, millor funcionarà el sistema. Tot i que us deixem a vosaltres la política de creació del vector de característiques us donem una pista: una possible estratègia és agafar aquelles paraules que apareixen entre en un 10 i un 50 percent del total de piludades (sense tenir en compte la categoria). És a dir, paraules que no són ni massa freqüents ni massa poc. Podeu experimentar variant aquests valors.\n",
    "\n",
    "Una altra estratègia interessant és eliminar \"stop words\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 5: \n",
    "\n",
    "+ Feu una funció ``topNwords`` que crei un diccionari amb les N paraules més representatives (les que apareixen amb més freqüència) de cadascuna de les categories de *cyberbullying* (Tenint en compte que haureu tret aquelles paraules que apareixen en la majoria de piulades, així com també, les que únicament apareixen en un conjunt molt petit de piulades). Una pista de que aneu ben encaminats es que per cadascuna de les categories de *cyberbullying* obtingueu paraules rellevants per aquesta. Si no es així, vol dir que heu d'incrementar el nombre de paraules a saltar (*skip_top*).\n",
    "\n",
    "El resultat serà un diccionari tipus (no necessàriament amb aquest valors):\n",
    "\n",
    "```python\n",
    "{\n",
    "    'age': ['school', 'high', ...],\n",
    "    ...\n",
    "    'religion': ['muslims', 'christian',...]\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "+ Experimenteu omplint la llista `skip_top` amb aquelles paraules que penseu no tenen significat o relevancia per definir cada categoria. Podeu buscar informació sobre **stop words** a internet i definir varies llistes fins que penseu que obteniu una bona representació de paraules per categoria de `cyberbullying`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "def topNwords(df, words, N, skip=[]):\n",
    "    \"\"\"\n",
    "    :param df: DataFrame amb les piulades i la informació associada\n",
    "    :param words: diccionari amb les paraules i la seva frequencia\n",
    "    :param N: número de paraules més representatives que volem considerar\n",
    "    :return : Diccionari amb el format {categoria1: llista_top_words_cat_1,  \n",
    "                                        categoria2: llista_top_words_cat_2, ...} \n",
    "    \"\"\"\n",
    "    top_words=dict()\n",
    "\n",
    "    for category in df['cyberbullying_type'].unique():\n",
    "        df_filtered = df[df['cyberbullying_type'] == category]\n",
    "\n",
    "        category_words = {}\n",
    "        for word, info in words[category].items():\n",
    "            if skip is not []:\n",
    "                if word not in skip : category_words[word] = info['n_ocur']\n",
    "            else:\n",
    "                if 0.1 <= (info['n_piu'] / df_filtered.shape[0]) <= 0.7:  category_words[word] = info['n_ocur']\n",
    "\n",
    "\n",
    "\n",
    "        sorted_words = sorted(category_words.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        top_words[category] = [word for word, freq in sorted_words[:N]]\n",
    "\n",
    "\n",
    "    return top_words\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Vector de Característiques\n",
    "\n",
    "#### EXERCICI 6: \n",
    "\n",
    "Creeu el vector de característiques necessari per a fer l’entrenament del Naïve Bayes amb la funció ``create_features``.\n",
    "\n",
    "El resultat serà un diccionari tipus (no necessàriament amb aquest valors):\n",
    "\n",
    "```python\n",
    "{\n",
    "    0: np.array([0, 1, 1, 0, ...]),\n",
    "    1: np.array([0, 1, 1, 1, ...]),\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "def create_features(df, top_words):\n",
    "    \"\"\"\n",
    "    :params df: DataFrame amb les piulades i la informació associada\n",
    "    :params top_words: ha de ser el diccionari que retorna topNWords\n",
    "    :return : diccionari o pd.Series que conté un np.array per a \n",
    "        cadascuna de les piulades amb el vector de característiques corresponent.\n",
    "    \"\"\"\n",
    "    all_worlds = [word for category, words in top_words.items() for word in words]\n",
    "    all_worlds = set(all_worlds)\n",
    "    n_words = len(all_worlds)\n",
    "    valid_idx = df.index.tolist()\n",
    "    dict_feat_vector = {}\n",
    "    for idx, piulada in enumerate(df['tweet_text']):\n",
    "        vect_piulada = np.zeros(n_words, dtype=int)\n",
    "\n",
    "        piulada_standardized = [standardize(word) for word in piulada.split()]\n",
    "\n",
    "        for pos, top_word in enumerate(all_worlds):\n",
    "            if top_word in piulada_standardized: vect_piulada[pos] = 1\n",
    "\n",
    "        dict_feat_vector[valid_idx[idx]] = vect_piulada\n",
    "\n",
    "\n",
    "    return dict_feat_vector"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "N = 20 # Aquest parametre el podem canviar i fer proves per avaluar quin és el millor valor.\n",
    "words_categories = count_words_categories(df_tweets_train)\n",
    "\n",
    "top_words = topNwords(df_tweets_train, words_categories, N)\n",
    "dict_feat_vector = create_features(df_tweets_train, top_words)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "len(dict_feat_vector)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. El classificador Naïve Bayes\n",
    "\n",
    "Un cop tenim una representació necessitem un procés d'aprenentatge que ens permeti passar de la descripció a una categoria. \n",
    "En aquest lliurament farem servir el classificador Naïve Bayes. \n",
    "Aquest classificador forma part de la família de classificadors probabilístics. \n",
    "La sortida d'un classificador probabilístic és un valor de probabilitat donat un exemple per cadascuna de les categories. \n",
    "La decisió final correspon a la categoria amb més probabilitat. \n",
    "\n",
    "\n",
    "Els classificadors probabilistics Bayesians es basen en el teorema de Bayes per realitzar els càlculs per estimar la probabilitat condicionada $p(y|x)$, on $y$ és la categoria i $\\mathbf x$ les característiques de l'exemple.\n",
    "\n",
    "La fòrmula de Bayes és fàcil de deduir. Sabem que:\n",
    "\n",
    "$$ p(\\mathbf x,y) = p(\\mathbf x|y)p(y) = p(y|\\mathbf x)p(\\mathbf x)$$\n",
    "d'on podem extreure que: \n",
    "$$ p(y|\\mathbf x) = \\frac{p(\\mathbf x|y)p(y)}{p(\\mathbf x)}$$\n",
    "\n",
    "\n",
    "La millor categoria serà la que fa màxima la probabilitat $ p(y|\\mathbf x)$ i per comparar aquests valors i saber quin és el màxim no cal calcular $p(\\mathbf x)$ (que és constant). Per tant, considerarem que:\n",
    "$$ p(y|\\mathbf x) \\approx p(y) · p(\\mathbf x|y)$$\n",
    "\n",
    "\n",
    "Les deduccions fins a aquest punt són vàlides per la majoria de classificadors Bayesians. \n",
    "Naïve Bayes es distingeix de la resta perquè imposa una condició encara més restrictiva. \n",
    "Considerem $\\mathbf x=(x_1, \\cdots, x_n)$ un conjunt d'$N$ variables aleatòries (en el nostre cas, les paraules seleccionades). \n",
    "Naïve Bayes assumeix que la probabilitat de la presència d'una paraula en una piulada és independent de la presència d'una altra i per tant podem escriure:\n",
    "$$p(x_1,x_2,...,x_N | y) = p(x_1|y)p(x_2|y)...p(x_N|y)$$\n",
    "\n",
    "\n",
    "Podem interpretar l'anterior equació de la següent forma: La probabilitat de que una piuldad descrita pel vector de característiques (0,1,0,1,1,1) sigui de la classe \"gender\" és proporcional al producte de la probabilitat que la primera paraula del vector no aparegui en les piulades sobre \"gender\" per la probabilitat que la segona paraula sí que hi aparegui, etc.\n",
    "\n",
    "\n",
    "**Estimant les probabilitats marginals condicionades**\n",
    "\n",
    "L'últim pas que ens queda és trobar el valor de les probabilitats condicionades. \n",
    "Farem servir la representació de $0$'s i $1$'s indicant que la paraula no apareix (0) o sí apareix (1) a la piulada. \n",
    "\n",
    "Per trobar el valor de la probabilitat condicionada farem servir una aproximació freqüentista a la probabilitat. \n",
    "Això vol dir que calcularem la freqüència d'aparició de cada paraula per a cada categoria. \n",
    "Aquest càlcul es fa dividint el nombre de piulades de la categoria en que apareix la paraula pel nombre total de piulades d'aquella categoria. \n",
    "\n",
    "En general:\n",
    "$$p(x = \\text{\"school\"} | y = C)= \\frac{A}{B} $$\n",
    "on $A$ és el número de piulades de la categoria $C$ on hi apareix la paraula 'school' i $B$ és el número total de piulades de la categoria $C$.\n",
    "\n",
    "\n",
    "#### Punts delicats a tenir en compte.\n",
    "\n",
    "**El problema de la probabilitat 0**\n",
    "\n",
    "Si us hi fixeu bé, la probabilitat pot ser 0!!  Això vol dir, que si en una piulada no hi apareix una paraula, no pot ser classificada com cap tipus de *cyber bullying* (la presència del 0 al producte fa que el resultat sigui 0).\n",
    "\n",
    "No sembla raonable que s'assigni o no en aquesta categoria segons si en la piulada hi apareix o no una única paraula. \n",
    "Per tant, el que s'acostuma a fer és donar una baixa probabilitat en comptes de zero. \n",
    "\n",
    "Una de les possibles solucions es fer servir la correcció de Laplace. Seguint l'exemple anterior la correcció de Laplace és:\n",
    "\n",
    "$$p(x= \\text{\"school\"} | y = 'C' ) = \\frac{A+1}{B+M}$$ \n",
    "\n",
    "on $M$ és el nombre de categories.\n",
    "\n",
    "**El problema de l'\"underflow\"**\n",
    "\n",
    "La valor que hem de calcular en el Naive Bayes és el resultat d'un producte. \n",
    "El nombre de caractéristiques del vector és el nombre de termes del producte. \n",
    "Aquests nombres són iguals o menors a 1 i n'hi ha molts, si els multipliquem entre ells el resultat serà massa petit per a representar-lo en un nombre de punt flotant i el càlcul acabarà sent reduït a zero. \n",
    "\n",
    "Per solucionar aquest problema en comptes d'operar fent multiplicacions, se sol passar a l'escala logarítmica i allà operar fent servir sumes en comptes de multiplicacions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 7: \n",
    "\n",
    "+ Implementeu la funció d'aprenentatge del classificador Naïve Bayes (funció ``naive_bayes_learn()``) que retorni un diccionari amb estructura `{categoria: [P0, ..., PN]}` on la llista representa la probabilitat\n",
    "marginal condicionada de cada paraula del vector de característiques per la categoria corresponent. \n",
    "\n",
    "+ Implementeu la funció ``naive_bayes`` que implementa el classificador. Noteu que aquesta funció está guiada i només haureu d'emplenar els espais on hem posat tres punts suspensius \"#···\".  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def naive_bayes_learn(df, feats):\n",
    "    \"\"\"\n",
    "    :params df: DataFrame amb les piulades i la informació associada\n",
    "    :params feats: vector de característiques de cada piulada\n",
    "    :return : probabilitats marginals condicionades\n",
    "    \"\"\"\n",
    "    probs = {}\n",
    "    \n",
    "    # Obtenim les categories de 'cyberbullying_type'\n",
    "    cyberbullying_type  = df['cyberbullying_type'].unique()\n",
    "    M = len(cyberbullying_type)\n",
    "\n",
    "    # Iterem sobre els índexs i els valors dels vectors de característiques\n",
    "    for index, values in feats.items():\n",
    "        \n",
    "        # Obtenim la piulada i la seva categoria\n",
    "        tweet = df.loc[index]\n",
    "        category = tweet[\"cyberbullying_type\"]\n",
    "        \n",
    "        # Afegim els valors al diccionari de probabilitats\n",
    "        if category not in probs.keys():\n",
    "            probs[category] = values\n",
    "        else:\n",
    "            probs[category] += values\n",
    "            \n",
    "    # Calculem les probabilitats condicionades per cada categoria\n",
    "    for category in cyberbullying_type:\n",
    "        \n",
    "        A = probs[category] # Sumatori de les característiques per la categoria\n",
    "        B = len(df[df[\"cyberbullying_type\"] == category]) # Nombre de piulades en la categoria\n",
    "        \n",
    "        # Probabilitat condicional amb suavització additiva\n",
    "        probs[category] = (A+1)/(B+M)\n",
    "    \n",
    "\n",
    "    return probs\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sys\n",
    "from IPython import embed\n",
    "def naive_bayes(df_train, feat_train, feat_test=None, df_test=None):\n",
    "    \"\"\"\n",
    "    Funció que implementa el clasificador Naive_Bayes.\n",
    "\n",
    "    Si df_test no és None, ha de calcular l'encert sobre les dades de test. És a dir,\n",
    "    després de classificar feat_test ha de comparar la classificació amb la classe\n",
    "    real i dir (print) quin percentatge d'encert ha obtingut.\n",
    "\n",
    "    :param df_train: DataFrame amb les piulades que s'utilitzaran per l'entrenament\n",
    "    :param feat_train: Diccionari amb els vectors de caracteristiques de cada tweet de l'entrenament\n",
    "    :param feat_test: Diccionari amb els vectors de caracteristiques de cada tweet de test\n",
    "    :param df_test: DataFrame amb les piulades que s'utilitzaran pel test\n",
    "\n",
    "    :return : Una serie on l'index correspon amb els indexos de df_test i els valors són la\n",
    "              classificació retornada per Naive Bayes\n",
    "    \"\"\"\n",
    "    probs = naive_bayes_learn(df_train, feat_train)\n",
    "\n",
    "    # Probabilitats totals per categoria\n",
    "    p_total = len(df_train)\n",
    "    p_of_cat = df_train['cyberbullying_type'].value_counts() / p_total\n",
    "\n",
    "    def eachFeats(row):\n",
    "        id, feat = row\n",
    "        p_max = -float('inf')\n",
    "        feat = np.array(feat)\n",
    "        p_cat = None\n",
    "\n",
    "        # Calcula la probabilitat màxima per cada categoria\n",
    "        for category in probs:\n",
    "            # Speed up by using numpy\n",
    "            # inv is the inverse of features, 0 where 1 and 1 where 0\n",
    "\n",
    "            # Probs * feats is the probability of being there, while\n",
    "            # inv - inv * feat = 1 - (0, 1, 0... inverses) * probs, probability of not being there\n",
    "            \n",
    "            # Sum of logs [vs] underflow caused by mul of probs\n",
    "            prob_category = np.log(p_of_cat[category])\n",
    "            prob_category += np.sum(\n",
    "                np.log(probs[category]) * feat + np.log(1 - probs[category]) * (1 - feat)\n",
    "            )\n",
    "\n",
    "            # Take the max, do it now to avoid extra-loops\n",
    "            if prob_category > p_max:\n",
    "                p_max = prob_category\n",
    "                p_cat = category\n",
    "\n",
    "        return id, p_cat\n",
    "\n",
    "    data = map(eachFeats, feat_test.items())\n",
    "    data = pd.Series(dict(data)).reindex(df_test.index)\n",
    "    correct = data == df_test['cyberbullying_type']\n",
    "    print(\"Accuracy: {}\".format(correct.sum() / correct.size))\n",
    "\n",
    "    return correct.sum() / correct.size"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "N = 20 # Aquest parametre el podeu canviar i fer proves per avaluar quin és el millor valor.\n",
    "\n",
    "words_topics = count_words_categories(df_tweets_train)\n",
    "top_words = topNwords(df_tweets_train, words_topics, N)\n",
    "print(top_words)\n",
    "feat_train = create_features(df_tweets_train, top_words)\n",
    "feat_test = create_features(df_tweets_test, top_words)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "accuracy = naive_bayes(df_tweets_train, feat_train, feat_test, df_tweets_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haurieu d'obtenir una precisió del 67-70%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXERCICI 8: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El possible procediment per tal d'aconseguir una major precisió seria el següent:\n",
    "+ Es poden implementar diverses maneres d'escollir quines *stopwords* volem eliminar en les piuldades per a que no formin part dels vectors de característiques.\n",
    "+ Avaluar quin conjunt d'*stopwords* retorna una precisió major per a N=40. \n",
    "+ Un cop tinguem el vector d'*stopwords* amb millors resultats, el testejarem per a diferents $N$'s per a veure quina és el nombre de *stopwords* òptim.\n",
    "\n",
    "Feu una cerca a Intenet per trobar més estratègies i intenteu millorar l'*accuracy* que heu acosneguit fins ara.\n",
    "\n",
    "Amb això podeu arribar a precisions superiors al 80%."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def define_skiptop():\n",
    "    nltk.download('stopwords')\n",
    "    # Obtener la lista de stop words en inglés\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return stop_words"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:12:50.892301Z",
     "start_time": "2024-12-15T15:12:50.260674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_idf_from_counts(word_counts, total_tweets):\n",
    "    \"\"\"\n",
    "    Calcula l'IDF a partir del diccionari generat per `count_words`.\n",
    "    :param word_counts: Diccionari amb format {word: {'n_ocur': valor, 'n_piu': valor}, ...}\n",
    "    :param total_tweets: Nombre total de tweets (documents)\n",
    "    :return: Diccionari amb format {word: IDF_value, ...}\n",
    "    \"\"\"\n",
    "    # Convertim el diccionari de word_counts a un DataFrame per utilitzar operacions vectoritzades\n",
    "    df = pd.DataFrame.from_dict(word_counts, orient='index')\n",
    "\n",
    "    # Calculem l'IDF de manera vectoritzada: log(N / (1 + n_piu))\n",
    "    df['idf'] = np.log(total_tweets / (1 + df['n_piu']))\n",
    "\n",
    "    # Convertim el resultat de nou a un diccionari\n",
    "    return df['idf'].to_dict()\n",
    "\n",
    "def extract_stop_words_from_idf(idf_values, idf_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Genera una llista de paraules stop words basada en el llindar de l'IDF.\n",
    "    :param idf_values: Diccionari amb format {word: IDF_value, ...}\n",
    "    :param idf_threshold: Llindar per determinar si una paraula és comuna (baix IDF)\n",
    "    :return: Conjunt de paraules considerades com a stop words\n",
    "    \"\"\"\n",
    "    # Filtrar paraules amb IDF menor al llindar i amb longitud superior a 2\n",
    "    stop_words = {word for word, idf in idf_values.items() if idf < idf_threshold and len(word) > 2}\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "# Total de tweets per calcular les estadístiques\n",
    "total_tweets = len(df_tweets_train)\n",
    "\n",
    "# Pas 1: Comptar paraules i les seves estadístiques\n",
    "word_counts = count_words(df_tweets_train)\n",
    "\n",
    "# Pas 2: Calcular l'IDF a partir dels comptatges\n",
    "idf_values = compute_idf_from_counts(word_counts, total_tweets)\n",
    "\n",
    "# Generar la llista de Stop Words basada en l'IDF\n",
    "idf_threshold = 1.8  # Llindar, pots ajustar-lo segons les teves necessitats\n",
    "stop_words = extract_stop_words_from_idf(idf_values, idf_threshold)\n",
    "\n",
    "# Unir les stop words generades amb les estàndard en anglès\n",
    "stop_word_english = define_skiptop()\n",
    "skip_words = stop_words.union(set(stop_word_english))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 451
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:11:36.293241Z",
     "start_time": "2024-12-15T15:11:31.772937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_words = topNwords(df_tweets_train, words_topics, 40, skip=skip_words)\n",
    "\n",
    "feat_train = create_features(df_tweets_train, top_words)\n",
    "feat_test = create_features(df_tweets_test, top_words)\n",
    "accuracy = naive_bayes(df_tweets_train, feat_train, feat_test, df_tweets_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7880280951881748\n"
     ]
    }
   ],
   "execution_count": 447
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:11:44.926269Z",
     "start_time": "2024-12-15T15:11:39.030231Z"
    }
   },
   "source": [
    "top_words = topNwords(df_tweets_train, words_topics, 80, skip=skip_words)\n",
    "feat_train = create_features(df_tweets_train, top_words)\n",
    "feat_test = create_features(df_tweets_test, top_words)\n",
    "\n",
    "accuracy = naive_bayes(df_tweets_train, feat_train, feat_test, df_tweets_test)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7925359052311564\n"
     ]
    }
   ],
   "execution_count": 448
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:59:32.311699Z",
     "start_time": "2024-12-15T14:59:23.380201Z"
    }
   },
   "source": [
    "top_words = topNwords(df_tweets_train, words_topics, 200, skip=skip_words)\n",
    "\n",
    "feat_train = create_features(df_tweets_train, top_words)\n",
    "feat_test = create_features(df_tweets_test, top_words)\n",
    "\n",
    "accuracy = naive_bayes(df_tweets_train, feat_train, feat_test, df_tweets_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8050110074431283\n"
     ]
    }
   ],
   "execution_count": 418
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T15:12:22.749324Z",
     "start_time": "2024-12-15T15:11:55.988903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_words = topNwords(df_tweets_train, words_topics, 800, skip=skip_words)\n",
    "\n",
    "feat_train = create_features(df_tweets_train, top_words)\n",
    "feat_test = create_features(df_tweets_test, top_words)\n",
    "\n",
    "accuracy = naive_bayes(df_tweets_train, feat_train, feat_test, df_tweets_test)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.812663801237027\n"
     ]
    }
   ],
   "execution_count": 450
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
